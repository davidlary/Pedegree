#!/usr/bin/env python3
"""
IntelligentLLMRouter.py - Intelligent Model Selection Router

Automatically selects the best LLM for each task based on:
- Task type and complexity
- Token length requirements  
- Cost constraints
- Performance requirements
- Local vs hosted preferences

Generated by GetAvailableModels.py
"""

import re
import json
import subprocess
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

class IntelligentLLMRouter:
    def __init__(self, models_data_file: str = 'available_models.json'):
        """Initialize router with model data."""
        try:
            with open(models_data_file, 'r') as f:
                self.models_data = json.load(f)
        except FileNotFoundError:
            print(f"Warning: {models_data_file} not found. Using default configuration.")
            self.models_data = self._get_default_models()
        
        self.routing_rules = self.models_data.get('routing_recommendations', {})
        
        # Ollama model mappings
        self.ollama_models = {
            'llama-3-8b': 'llama3.1:8b',
            'llama3-8b': 'llama3.1:8b', 
            'mistral-7b': 'mistral:7b',
            'code-llama-7b': 'codellama:7b',
            'phi-3-mini': 'phi3:mini'
        }
        
    def route_request(self, 
                     prompt: str, 
                     task_type: Optional[str] = None,
                     max_cost_per_1k: Optional[float] = None,
                     prefer_local: bool = False,
                     require_multimodal: bool = False,
                     context_length_estimate: Optional[int] = None,
                     use_ollama: bool = True) -> Dict[str, Any]:
        """
        Route a request to the best available model.
        
        Args:
            prompt: The input prompt/query
            task_type: Optional explicit task type
            max_cost_per_1k: Maximum cost per 1K tokens
            prefer_local: Prefer local models when possible
            require_multimodal: Require vision/multimodal capabilities
            context_length_estimate: Estimated context length needed
            use_ollama: Use Ollama for local model execution
            
        Returns:
            Dict with recommended model and routing reasoning
        """
        
        # Detect task type if not provided
        if not task_type:
            task_type = self._detect_task_type(prompt)
        
        # Estimate context length if not provided
        if not context_length_estimate:
            context_length_estimate = self._estimate_context_length(prompt)
        
        # Get routing recommendation
        recommendation = self._get_task_recommendation(task_type)
        
        # Apply constraints
        final_model = self._apply_constraints(
            recommendation, 
            max_cost_per_1k=max_cost_per_1k,
            prefer_local=prefer_local,
            require_multimodal=require_multimodal,
            context_length=context_length_estimate,
            use_ollama=use_ollama
        )
        
        # Check if model is local and add Ollama execution info
        execution_info = {}
        if use_ollama and self._is_local_model(final_model):
            ollama_model = self.ollama_models.get(final_model, final_model)
            execution_info = {
                'execution_type': 'ollama',
                'ollama_model': ollama_model,
                'command': f'ollama run {ollama_model}',
                'available': self._check_ollama_model_available(ollama_model)
            }
        else:
            execution_info = {
                'execution_type': 'api',
                'provider': self._get_model_provider(final_model)
            }
            
        return {
            'recommended_model': final_model,
            'detected_task_type': task_type,
            'estimated_context_length': context_length_estimate,
            'routing_reasoning': self._explain_choice(final_model, task_type, recommendation),
            'alternatives': self._get_alternatives(recommendation, final_model),
            'execution_info': execution_info,
            'timestamp': datetime.now().isoformat()
        }
    
    def _detect_task_type(self, prompt: str) -> str:
        """Detect task type from prompt content."""
        prompt_lower = prompt.lower()
        
        # Physics/STEM detection
        physics_keywords = ['physics', 'thermodynamics', 'quantum', 'mechanics', 'energy', 'force', 'acceleration']
        if any(keyword in prompt_lower for keyword in physics_keywords):
            return 'physics_stem'
        
        # Code detection
        code_keywords = ['function', 'class', 'def ', 'import', 'return', 'if __name__', 'print(', 'console.log']
        if any(keyword in prompt_lower for keyword in code_keywords):
            return 'code_generation'
        
        # Multimodal detection
        multimodal_keywords = ['image', 'photo', 'picture', 'diagram', 'chart', 'visual', 'screenshot']
        if any(keyword in prompt_lower for keyword in multimodal_keywords):
            return 'multimodal_vision'
        
        # Long context detection (heuristic based on prompt length)
        if len(prompt) > 10000:
            return 'long_context'
        
        # Simple/fast task detection
        if len(prompt) < 200 and any(word in prompt_lower for word in ['what', 'how', 'when', 'where', 'why']):
            return 'fast_simple_tasks'
        
        return 'general_purpose'
    
    def _estimate_context_length(self, prompt: str) -> int:
        """Estimate context length needed (rough tokens = chars/4)."""
        return len(prompt) // 4
    
    def _get_task_recommendation(self, task_type: str) -> Dict[str, str]:
        """Get base recommendation for task type."""
        task_routing = self.routing_rules.get('task_based_routing', {})
        
        if task_type in task_routing:
            return task_routing[task_type]
        
        # Default recommendation
        return {
            'primary': 'gpt-4o',
            'fallback': 'claude-3-sonnet-20240229', 
            'local_option': 'llama-3-8b',
            'reasoning': 'General purpose task, using balanced models'
        }
    
    def _apply_constraints(self, 
                          recommendation: Dict[str, str],
                          max_cost_per_1k: Optional[float] = None,
                          prefer_local: bool = False,
                          require_multimodal: bool = False,
                          context_length: int = 0,
                          use_ollama: bool = True) -> str:
        """Apply constraints to select final model."""
        
        candidates = [
            recommendation.get('primary'),
            recommendation.get('fallback'),
            recommendation.get('local_option')
        ]
        
        # Filter out None values
        candidates = [c for c in candidates if c]
        
        if prefer_local:
            local_candidate = recommendation.get('local_option')
            if local_candidate and self._check_model_constraints(local_candidate, max_cost_per_1k, require_multimodal, context_length):
                return local_candidate
        
        if require_multimodal:
            multimodal_models = ['gpt-4o', 'gemini-pro-vision', 'grok-vision-beta']
            for model in multimodal_models:
                if model in candidates and self._check_model_constraints(model, max_cost_per_1k, require_multimodal, context_length):
                    return model
        
        # Check candidates in order
        for candidate in candidates:
            if self._check_model_constraints(candidate, max_cost_per_1k, require_multimodal, context_length):
                return candidate
        
        # Fallback to cheapest available model
        return 'gpt-3.5-turbo'
    
    def _check_model_constraints(self, 
                                model_id: str,
                                max_cost_per_1k: Optional[float] = None,
                                require_multimodal: bool = False,
                                context_length: int = 0) -> bool:
        """Check if model meets constraints."""
        
        # Find model info across all providers
        model_info = None
        for provider_models in self.models_data.get('providers', {}).values():
            if model_id in provider_models:
                model_info = provider_models[model_id]
                break
        
        if not model_info:
            # Check local models
            local_models = self.models_data.get('local_models', {})
            if model_id in local_models:
                model_info = local_models[model_id]
        
        if not model_info:
            return False
        
        # Check cost constraint
        if max_cost_per_1k is not None:
            pricing = model_info.get('pricing', {})
            input_cost = pricing.get('input', 0)
            if isinstance(input_cost, (int, float)) and input_cost > max_cost_per_1k:
                return False
        
        # Check context length
        model_context = model_info.get('context_window', 0)
        if isinstance(model_context, int) and context_length > model_context:
            return False
        
        # Check multimodal requirement
        if require_multimodal:
            multimodal_models = ['gpt-4o', 'gemini-pro-vision', 'grok-vision-beta']
            if model_id not in multimodal_models:
                return False
        
        return True
    
    def _explain_choice(self, chosen_model: str, task_type: str, recommendation: Dict[str, str]) -> str:
        """Explain why this model was chosen."""
        base_reasoning = recommendation.get('reasoning', 'Standard routing logic')
        return f"Selected {chosen_model} for {task_type} task. {base_reasoning}"
    
    def _get_alternatives(self, recommendation: Dict[str, str], chosen_model: str) -> List[str]:
        """Get alternative model options."""
        alternatives = []
        for key in ['primary', 'fallback', 'local_option']:
            model = recommendation.get(key)
            if model and model != chosen_model:
                alternatives.append(model)
        return alternatives
    
    def _is_local_model(self, model_id: str) -> bool:
        """Check if model is a local model."""
        local_model_patterns = ['llama', 'mistral', 'phi', 'code-llama', 'gemma', 'falcon']
        return any(pattern in model_id.lower() for pattern in local_model_patterns)
    
    def _get_model_provider(self, model_id: str) -> str:
        """Get the provider for a model."""
        if 'gpt' in model_id.lower() or 'o1' in model_id.lower():
            return 'openai'
        elif 'claude' in model_id.lower():
            return 'anthropic'
        elif 'gemini' in model_id.lower():
            return 'google'
        elif 'grok' in model_id.lower():
            return 'xai'
        elif self._is_local_model(model_id):
            return 'local'
        else:
            return 'unknown'
    
    def _check_ollama_model_available(self, ollama_model: str) -> bool:
        """Check if Ollama model is installed and available."""
        try:
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=10)
            return ollama_model in result.stdout
        except (subprocess.SubprocessError, FileNotFoundError):
            return False
    
    def query_ollama(self, model: str, prompt: str, stream: bool = False) -> str:
        """Query a local model via Ollama."""
        ollama_model = self.ollama_models.get(model, model)
        
        if not self._check_ollama_model_available(ollama_model):
            raise ValueError(f"Ollama model {ollama_model} is not installed. Run: ollama pull {ollama_model}")
        
        try:
            cmd = ['ollama', 'run', ollama_model, prompt]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
            
            if result.returncode == 0:
                return result.stdout.strip()
            else:
                raise RuntimeError(f"Ollama error: {result.stderr}")
                
        except subprocess.TimeoutExpired:
            raise TimeoutError("Ollama query timed out after 120 seconds")
        except FileNotFoundError:
            raise RuntimeError("Ollama is not installed or not in PATH")
    
    def get_available_local_models(self) -> List[str]:
        """Get list of available local models via Ollama."""
        try:
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')[1:]  # Skip header
                return [line.split()[0] for line in lines if line.strip()]
            return []
        except (subprocess.SubprocessError, FileNotFoundError):
            return []
    
    def _get_default_models(self) -> Dict[str, Any]:
        """Default model configuration if data file not available."""
        return {
            'routing_recommendations': {
                'task_based_routing': {
                    'physics_stem': {
                        'primary': 'grok-4',
                        'fallback': 'claude-3-opus-20240229',
                        'local_option': 'mixtral-8x7b',
                        'reasoning': 'STEM-focused models'
                    },
                    'code_generation': {
                        'primary': 'gpt-4o',
                        'fallback': 'claude-3-5-sonnet-20241022',
                        'local_option': 'code-llama-7b',
                        'reasoning': 'Code-optimized models'
                    },
                    'general_purpose': {
                        'primary': 'gpt-4o',
                        'fallback': 'claude-3-sonnet-20240229',
                        'local_option': 'llama-3-8b',
                        'reasoning': 'Balanced general purpose models'
                    }
                }
            }
        }

# Example usage
if __name__ == "__main__":
    router = IntelligentLLMRouter()
    
    # Example routing requests
    # Show available local models
    print("üîç Available Local Models via Ollama:")
    local_models = router.get_available_local_models()
    for model in local_models:
        print(f"  ‚úÖ {model}")
    print()
    
    test_prompts = [
        ("Explain quantum mechanics and thermodynamics", None, False),
        ("Write a Python function to sort a list", None, True),
        ("What's the weather like?", None, True),
        ("def fibonacci(n):\n    if n <= 1:\n        return n", "code_generation", True)
    ]
    
    print("üß† Intelligent LLM Router - Test Results")
    print("=" * 80)
    
    for prompt, task_type, prefer_local in test_prompts:
        result = router.route_request(prompt, task_type=task_type, prefer_local=prefer_local)
        print(f"\nüìù Prompt: {prompt[:50]}...")
        print(f"üéØ Recommended: {result['recommended_model']}")
        print(f"üìä Task Type: {result['detected_task_type']}")
        print(f"üí≠ Reasoning: {result['routing_reasoning']}")
        print(f"‚öôÔ∏è  Execution: {result['execution_info']['execution_type']}")
        
        if result['execution_info']['execution_type'] == 'ollama':
            print(f"üñ•Ô∏è  Ollama Model: {result['execution_info']['ollama_model']}")
            print(f"üíª Command: {result['execution_info']['command']}")
            print(f"‚úÖ Available: {result['execution_info']['available']}")
        else:
            print(f"üåê Provider: {result['execution_info']['provider']}")
            
        # Example query for local models
        if (result['execution_info']['execution_type'] == 'ollama' and 
            result['execution_info']['available']):
            try:
                print("üöÄ Testing local query...")
                response = router.query_ollama(result['recommended_model'], 
                                             "Hello! Can you introduce yourself in one sentence?")
                print(f"ü§ñ Response: {response[:100]}...")
            except Exception as e:
                print(f"‚ùå Query failed: {str(e)[:50]}...")
        
        print("-" * 80)
